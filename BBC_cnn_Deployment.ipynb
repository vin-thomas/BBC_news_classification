{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BBC_cnn_Deployment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vin-thomas/BBC_news_classification/blob/main/BBC_cnn_Deployment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "353aMfJgCAxO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4561744b-d81d-4cf5-9b9d-cfa522f1d35c"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import torch\n",
        "import glob\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxfbdCGBDOZx"
      },
      "source": [
        "##**Data Download**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xt5HvEkvDNtV",
        "outputId": "040d75d6-5e1d-4771-da98-4efe569b4d0a"
      },
      "source": [
        "URL = \"http://mlg.ucd.ie/files/datasets/bbc-fulltext.zip\" \n",
        "!wget -P 'Data/' $URL"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-02-14 06:23:40--  http://mlg.ucd.ie/files/datasets/bbc-fulltext.zip\n",
            "Resolving mlg.ucd.ie (mlg.ucd.ie)... 137.43.93.132\n",
            "Connecting to mlg.ucd.ie (mlg.ucd.ie)|137.43.93.132|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2874078 (2.7M) [application/zip]\n",
            "Saving to: ‘Data/bbc-fulltext.zip’\n",
            "\n",
            "bbc-fulltext.zip    100%[===================>]   2.74M  3.72MB/s    in 0.7s    \n",
            "\n",
            "2022-02-14 06:23:41 (3.72 MB/s) - ‘Data/bbc-fulltext.zip’ saved [2874078/2874078]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ap3ya7hPDLNg"
      },
      "source": [
        "!unzip \"/content/Data/bbc-fulltext.zip\" -d 'Data/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NV5qLqIO6HPX"
      },
      "source": [
        "file_name_iter = glob.iglob('/content/Data/bbc/**/*.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiJiCOZX8bBG"
      },
      "source": [
        "def get_topic(file):\n",
        "  return file.split('/')[-2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H60LnL-Q9EOo"
      },
      "source": [
        "def get_content(file):\n",
        "  with open(file, 'rb') as f:\n",
        "    content = f.read()\n",
        "    return content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPHkEx6O837A"
      },
      "source": [
        "labels = []\n",
        "content = []\n",
        "\n",
        "for file in file_name_iter:\n",
        "  labels.append(get_topic(file))\n",
        "  content.append (get_content(file))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBumyr4Uszre"
      },
      "source": [
        "##**Prepare a word index, dictionary**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-UidBXu-Xep"
      },
      "source": [
        "word2idx= {}\n",
        "idx = 0\n",
        "n_l = []\n",
        "news_len =0\n",
        "\n",
        "for item in content:\n",
        "  word_list = word_tokenize(str(item))\n",
        "  n_l.append(len(word_list))\n",
        "  for word in word_list:\n",
        "    if word not in word2idx:\n",
        "      word2idx[word]= idx\n",
        "      idx += 1  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQWidJbtteFf"
      },
      "source": [
        "max_len_article = max(n_l)\n",
        "no_of_articles= len(content)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (max_len_article)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBN6uZeYhZGR",
        "outputId": "95b46779-6326-4768-9f2f-9bab16783a5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4862\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_TtNcmPqt77W",
        "outputId": "8d8bcccb-c4a9-42cd-ee0f-ce637249b1d2"
      },
      "source": [
        "dataset = np.zeros((no_of_articles, max_len_article), dtype= int)\n",
        "dataset.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2225, 4862)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1YWUkzzvGO2"
      },
      "source": [
        "i=0\n",
        "idx_list=[]\n",
        "for item in content:\n",
        "  word_list= word_tokenize(str(item))\n",
        "  for token in word_list:\n",
        "    if word2idx.get(token) is not None:\n",
        "      idx = word2idx.get(token)\n",
        "    else:\n",
        "      idx= 0\n",
        "    idx_list.append(idx)\n",
        "  pad_list = [0]*(max_len_article- len(idx_list))\n",
        "  idx_list = idx_list + pad_list\n",
        "  dataset[i]= idx_list\n",
        "  idx_list=[]\n",
        "  i+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset= torch.tensor(dataset)"
      ],
      "metadata": {
        "id": "lQlaf4XmbVfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import pickle\n",
        "# dictionary = open('/content/Data/word2idx', 'wb')\n",
        "# pickle.dump(word2idx, dictionary)\n",
        "# dictionary.close()"
      ],
      "metadata": {
        "id": "mLhoAQAqp4ZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(word2idx, '/content/Data/word2idx' )"
      ],
      "metadata": {
        "id": "gA1vx0XIszLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**DownLoad the pretrained vectors**"
      ],
      "metadata": {
        "id": "az_fGfIPCYSW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "URL = \"https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip\"\n",
        "FILE = \"fastText\"\n",
        "!wget -P $FILE $URL\n",
        "!unzip $FILE/crawl-300d-2M.vec.zip -d $FILE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKXjSKIgCXlB",
        "outputId": "bb66ed25-ccab-4700-d2ec-7f2ab01df348"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-02-14 06:24:46--  https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 172.67.9.4, 104.22.75.142, 104.22.74.142, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|172.67.9.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1523785255 (1.4G) [application/zip]\n",
            "Saving to: ‘fastText/crawl-300d-2M.vec.zip’\n",
            "\n",
            "crawl-300d-2M.vec.z 100%[===================>]   1.42G  30.6MB/s    in 54s     \n",
            "\n",
            "2022-02-14 06:25:40 (27.1 MB/s) - ‘fastText/crawl-300d-2M.vec.zip’ saved [1523785255/1523785255]\n",
            "\n",
            "Archive:  fastText/crawl-300d-2M.vec.zip\n",
            "  inflating: fastText/crawl-300d-2M.vec  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Load the pre-trained vectors and create the embedding matrix**"
      ],
      "metadata": {
        "id": "dM9k33G2D1Lq"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76xyIra75_dP"
      },
      "source": [
        "fin = open('/content/fastText/crawl-300d-2M.vec', 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
        "n, d = map(int, fin.readline().split())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = np.random.uniform(-0.25, 0.25, (len(word2idx), d))"
      ],
      "metadata": {
        "id": "JAqpB50VF_4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count =0\n",
        "for line in fin:\n",
        "  tokens = line.rstrip().split(' ')\n",
        "  word = tokens[0]\n",
        "  if word in word2idx:\n",
        "    embeddings[word2idx[word]] = np.array(tokens[1:], dtype=np.float32)"
      ],
      "metadata": {
        "id": "GUJQIp4fCtpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings= torch.tensor(embeddings)"
      ],
      "metadata": {
        "id": "VnGvFrZ-EzBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# embedding_dump = open('/content/Data/embeddings', 'wb')\n",
        "# pickle.dump(embeddings, embedding_dump)\n",
        "# embedding_dump.close()"
      ],
      "metadata": {
        "id": "b03O9rm-0OCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(embeddings, '/content/Data/embeddings' )"
      ],
      "metadata": {
        "id": "OzkA5NlstY8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpqfQcbO6Ykn",
        "outputId": "f35a1b06-3e58-4463-f2fb-df4a47db56d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([50108, 300])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have an embedding array wich has the vector for each word in our dictionary. Further, we have 'dataset' which gives the word index for each article"
      ],
      "metadata": {
        "id": "EfSylroUKqA0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Create Pytorch Dataloaders**"
      ],
      "metadata": {
        "id": "-7pwPiJn81q_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels, uniques = pd.factorize(labels)"
      ],
      "metadata": {
        "id": "ja01f73FLrBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels, uniques"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxSmBUdw8jl1",
        "outputId": "28beb8ca-92fc-4515-b82e-86d68a57860a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0, 0, 0, ..., 4, 4, 4]),\n",
              " array(['politics', 'tech', 'business', 'entertainment', 'sport'],\n",
              "       dtype=object))"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "uniques[4]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "NhR_LOqX8jGB",
        "outputId": "3d6e0181-568b-4b57-ac74-8dffbaca27e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'sport'"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels = torch.from_numpy(labels)"
      ],
      "metadata": {
        "id": "mQPqI2dpzaUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Train Test Split\n",
        "train_inputs, val_inputs, train_labels, val_labels = train_test_split(dataset, labels, test_size=0.1, random_state=42)"
      ],
      "metadata": {
        "id": "QSRzd-a4_Rov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import (TensorDataset, DataLoader, RandomSampler,\n",
        "                              SequentialSampler)\n",
        "\n",
        "\n",
        "batch_size=50\n",
        "\n",
        "# Create DataLoader for training data\n",
        "train_data = TensorDataset(train_inputs, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create DataLoader for validation data\n",
        "val_data = TensorDataset(val_inputs, val_labels)\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "GElTJZYANoYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Device**"
      ],
      "metadata": {
        "id": "i5nHWuIP0TYg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():       \n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
        "    print('Device name:', torch.cuda.get_device_name(0))\n",
        "\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "282xUtFF2MEN",
        "outputId": "85983268-f6b7-4821-d9af-38ed627a2739"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "Device name: Tesla K80\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**CNN Model Architecture**"
      ],
      "metadata": {
        "id": "SlyHEG4XPmFB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "flKn6j0RPa4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "  def __init__(self,\n",
        "               pretrained_embedding,\n",
        "               ):\n",
        "\n",
        "      super(Model, self).__init__()\n",
        "      self.vocab_size, self.embed_dim = pretrained_embedding.shape\n",
        "      self.embedding = nn.Embedding.from_pretrained(pretrained_embedding,\n",
        "                                                          freeze=False)\n",
        "\n",
        "            \n",
        "        \n",
        "      # Defining first convolution layer with input_channels = 300, output_channels = 100, kernel_size = 3\n",
        "      self.conv1 = nn.Conv1d(in_channels=300, out_channels=100, kernel_size=3)\n",
        "      # Defining second convolution layer with input channels = 300, output channels = 100, kernel_size= 4\n",
        "      self.conv2 = nn.Conv1d(in_channels=300, out_channels=100, kernel_size=4)\n",
        "      # Defining third convolution layer with input channels =300, output channels = 100, kernel_size =5\n",
        "      self.conv3 = nn. Conv1d(in_channels= 300, out_channels= 100, kernel_size=5)\n",
        "\n",
        "            \n",
        "      # Define the Fully connected layers\n",
        "      # Each of the channels will generate an element, so (100+100+100 =300)\n",
        "      self.fc = nn.Linear(300, 5)\n",
        "        \n",
        "      self.dropout = nn.Dropout(0.5)\n",
        "        \n",
        "    \n",
        "  def forward(self, dataset):\n",
        "\n",
        "    print (dataset.shape)\n",
        "    x_embed = self.embedding(dataset).float()\n",
        "    print(x_embed.shape)\n",
        "\n",
        "    x_reshaped = x_embed.permute(0, 2, 1)\n",
        "    print(x_reshaped.shape)\n",
        "    \n",
        "    x1 = F.relu(self.conv1(x_reshaped))\n",
        "    x2 = F.relu(self.conv2(x_reshaped))\n",
        "    x3 = F.relu(self.conv2(x_reshaped))\n",
        "\n",
        "    x1_pool = F.max_pool1d(x1, kernel_size=x1.shape[2])\n",
        "    x2_pool = F.max_pool1d(x2, kernel_size=x2.shape[2])\n",
        "    x3_pool = F.max_pool1d(x3, kernel_size=x3.shape[2])\n",
        "\n",
        "    \n",
        "    x_fc = torch.cat([x1_pool.squeeze(dim=2), x2_pool.squeeze(dim=2), x3_pool.squeeze(dim=2)],\n",
        "                         dim=1)\n",
        "    logits = self.fc(self.dropout(x_fc))\n",
        "    \n",
        "    return logits"
      ],
      "metadata": {
        "id": "UkPC7E1srblJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Instatiate the CNN Model**"
      ],
      "metadata": {
        "id": "_6PRJFIDVAmG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model(embeddings)\n",
        "model = model.to(device)\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHQHMQmM0LT4",
        "outputId": "8273a9c3-8683-46d6-9527-25c279271da3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model(\n",
              "  (embedding): Embedding(50108, 300)\n",
              "  (conv1): Conv1d(300, 100, kernel_size=(3,), stride=(1,))\n",
              "  (conv2): Conv1d(300, 100, kernel_size=(4,), stride=(1,))\n",
              "  (conv3): Conv1d(300, 100, kernel_size=(5,), stride=(1,))\n",
              "  (fc): Linear(in_features=300, out_features=5, bias=True)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Optimizer and Loss Function**"
      ],
      "metadata": {
        "id": "GQcIqcvTT86_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Instantiate Adadelta optimizer\n",
        "optimizer = optim.Adadelta(model.parameters(),\n",
        "                               lr=.25,\n",
        "                               rho=0.95)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "a1hOPqFf1oNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Train**"
      ],
      "metadata": {
        "id": "Z7LtPbo9_qMQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate ():\n",
        "  eval_model = model\n",
        "  eval_model.eval\n",
        "  val_accuracy = 0\n",
        "  val_loss = 0\n",
        "  \n",
        "  for inputs, labels in val_dataloader:\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "    logits = eval_model(inputs)\n",
        "    \n",
        "    loss = criterion (logits, labels)\n",
        "    val_loss +=loss.item()\n",
        "    _, predicted = torch.max(logits, 1)\n",
        "    \n",
        "    correct = (predicted == labels).sum().item()\n",
        "    \n",
        "    val_accuracy +=correct\n",
        "\n",
        "  val_loss = val_loss/len(val_data)\n",
        "  val_accuracy = val_accuracy*100/len(val_data)\n",
        "\n",
        "  return val_loss, val_accuracy\n"
      ],
      "metadata": {
        "id": "tO94CX49beIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# No of Epochs\n",
        "epoch = 1\n",
        "\n",
        "# keeping the network in train mode\n",
        "model.train()\n",
        "train_losses,  train_accuracy = [], []\n",
        "\n",
        "# Loop for no of epochs\n",
        "for e in range(epoch):\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    # Iterate through all the batches in each epoch\n",
        "    for inputs, labels in train_dataloader:\n",
        "\n",
        "      # Convert the image and label to gpu for faster execution\n",
        "      inputs = inputs.to(device)\n",
        "      labels = labels.to(device)\n",
        "      \n",
        "      # Zero the parameter gradients\n",
        "      optimizer.zero_grad()\n",
        "      \n",
        "      # Passing the data to the model (Forward Pass)\n",
        "      outputs = model(inputs)\n",
        "      \n",
        "      # Calculating the loss\n",
        "      loss = criterion(outputs, labels)\n",
        "      train_loss += loss.item()\n",
        "\n",
        "      # Performing backward pass (Backpropagation)\n",
        "      loss.backward()\n",
        "\n",
        "      # optimizer.step() updates the weights accordingly\n",
        "      optimizer.step()\n",
        "\n",
        "      _, predicted = torch.max(outputs, 1)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "      val_loss, val_accuracy = evaluate()\n",
        "      break\n",
        "      \n",
        "    # Accuracy calculation\n",
        "    train_losses.append(train_loss/len (train_data))\n",
        "    train_accuracy.append(100 * correct/len(train_data))\n",
        "    print('epoch: {}, Train Loss:{:.6f} Train Accuracy: {:.2f} Validation loss: {:.2f} Validation accuracy: {:.2f} '.format(e+1,train_losses[-1], train_accuracy[-1], val_loss, val_accuracy))\n",
        "    "
      ],
      "metadata": {
        "id": "o8OOXvsX_ofN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model_dump = open('/content/Data/model', 'wb')\n",
        "# pickle.dump(model, model_dump)\n",
        "# model_dump.close()"
      ],
      "metadata": {
        "id": "yzWSxEbRAoWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), '/content/Data/model')"
      ],
      "metadata": {
        "id": "gTXL74tRuk1y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}